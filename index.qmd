---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: 
     mainfont: Times New Roman
execute:
  echo: false
  eval: true
---

# ðŸŒ³ Decision Tree Challenge - Feature Importance and Variable Encoding

```{python}
#| label: load-and-model-python
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load data
sales_data = pd.read_csv("salesPriceData.csv")

# Prepare model data (treating zipCode as numerical)
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build decision tree
#tree_model = DecisionTreeRegressor(max_depth=3, 
#                                  min_samples_split=20, 
#                                  min_samples_leaf=10, 
#                                  random_state=123)
#tree_model.fit(X_train, y_train)

#print(f"Model built with {tree_model.get_n_leaves()} terminal nodes")
```

```{python}
#| label: visualize-tree-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree
#plt.figure(figsize=(10, 6))
#plot_tree(tree_model, 
#          feature_names=X_train.columns,
#          filled=True, 
#          rounded=True,
#          fontsize=10,
#          max_depth=3)
#plt.title("Decision Tree (zipCode as Numerical)")
#plt.tight_layout()
#plt.show()
```
:::
```{python}
#| label: feature-importance-python
#| echo: false

# Extract and display feature importance
#importance_df = pd.DataFrame({
#    'Feature': X_train.columns,
#    'Importance': tree_model.feature_importances_
#}).sort_values('Importance', ascending=False)

#importance_df['Importance_Percent'] = (importance_df['Importance'] * 100).round(2)

# Check zipCode ranking
#zipcode_rank = importance_df[importance_df['Feature'] == 'zipCode'].index[0] + 1
#zipcode_importance = importance_df[importance_df['Feature'] == 'zipCode']['Importance_Percent'].iloc[0]
```

```{python}
#| label: importance-plot-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance
#plt.figure(figsize=(8, 5))
#plt.barh(range(len(importance_df)), importance_df['Importance'], 
#         color='steelblue', alpha=0.7)
#plt.yticks(range(len(importance_df)), importance_df['Feature'])
#plt.xlabel('Importance Score')
#plt.title('Feature Importance (zipCode as Numerical)')
#plt.gca().invert_yaxis()
#plt.tight_layout()
#plt.show()
```



```{python}
#| label: categorical-python
#| echo: true
#| include: false
# One-hot encode zipCode
import pandas as pd

# Create one-hot encoded zipCode
zipcode_encoded = pd.get_dummies(model_data['zipCode'], prefix='zipCode')
model_data_cat = pd.concat([model_data.drop('zipCode', axis=1), zipcode_encoded], axis=1)

# Split data
X_cat = model_data_cat.drop('SalePrice', axis=1)
y_cat = model_data_cat['SalePrice']
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=123)

# Build decision tree with one-hot encoded zipCode
tree_model_cat = DecisionTreeRegressor(max_depth=3, 
                                      min_samples_split=20, 
                                      min_samples_leaf=10, 
                                      random_state=123)
tree_model_cat.fit(X_train_cat, y_train_cat)

# Feature importance with one-hot encoded zipCode
importance_cat_df = pd.DataFrame({
    'Feature': X_train_cat.columns,
    'Importance': tree_model_cat.feature_importances_
}).sort_values('Importance', ascending=False)

importance_cat_df['Importance_Percent'] = (importance_cat_df['Importance'] * 100).round(2)

# Check zipCode features
zipcode_features = [col for col in X_train_cat.columns if col.startswith('zipCode')]
zipcode_importance = importance_cat_df[importance_cat_df['Feature'].isin(zipcode_features)]['Importance'].sum()
total_importance = importance_cat_df['Importance'].sum()
zipcode_percent = (zipcode_importance / total_importance * 100).round(2)
```

```{python}
#| label: visualize-tree-cat-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree with one-hot encoded zipCode
#plt.figure(figsize=(10, 6))
#plot_tree(tree_model_cat, 
#          feature_names=X_train_cat.columns,
#          filled=True, 
#          rounded=True,
#          fontsize=8,
#          max_depth=4)
#plt.title("Decision Tree (zipCode One-Hot Encoded)")
#plt.tight_layout()
#plt.show()
```

```{python}
#| label: importance-plot-cat-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance for categorical zipCode
#plt.figure(figsize=(8, 5))
#plt.barh(range(len(importance_cat_df)), #importance_cat_df['Importance'], 
#         color='darkgreen', alpha=0.7)
#plt.yticks(range(len(importance_cat_df)), #importance_cat_df['Feature'])
#plt.xlabel('Importance Score')
#plt.title('Feature Importance (zipCode #One-Hot Encoded)')
#plt.gca().invert_yaxis()
#plt.tight_layout()
#plt.show()
```

1. **Numerical vs Categorical Encoding:** There are two modelsin Python written above. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically?  Is zipcode and ordinal or non-ordinal variable?

::: {.callout-note appearance="minimal"}


####  Treat ZIP Code as a Categorical Variable 


Although ZIP codes look numeric, their ordering is arbitraryâ€”driven by postal system logistics rather than economic or geographic progression. Unlike ordinal variables, which have meaningful rankings such as low < medium < high, small < medium < large, or grade A < grade B < grade C, ZIP codes do not follow any inherent order. For this reason, ZIP codes are classified as **non-ordinal** categorical variables.

* Zip codes represent regions, not numbers.
* A ZIP code indicates a location / neighborhood, which often has strong relevance to real estate price.
* Categorical encoding (One-Hot, Target Encoding, etc.) captures regional differences properly.



:::

2. **R vs Python Implementation Differences:** When modelling zip code as a categorical variable, the output tree and feature importance would differ quite significantly had you used R as opposed to Python. Investigate why this is the case.  What does R offer that Python does not? Which language would you say does a better job of modelling zip code as a categorical variable? Can you quote the documentation at [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html) suggesting a weakness in the Python implementation? If so, please provide a quote from the documentation.

::: {.callout-note appearance="minimal"}


#### **Modelling ZIP Code as a Categorical Variable**

**R** provides a dedicated factor data type for representing categorical variables. Factors store categories efficiently by using an underlying integer vector, where each integer maps to a specific category level. These integers are paired with human-readable labels such as "male," "female," "red," "green," or "blue." Because R treats factors as true categorical entitiesâ€”rather than as separate dummy variablesâ€”it can model them more naturally in decision trees, allowing splits that group category levels in meaningful ways.

**Python** (scikit-learn) is more constrained: you must explicitly encode categories, which can distort the way splits happen (especially for high-cardinality variables like ZIP codes).

#### Quoting the Documentation (scikit-learn Weakness)

Here is a direct quote from the scikit-learn Decision Trees documentation that highlights the limitation:

`â€œAble to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now.â€ `


This clearly indicates that, despite decision trees being theoretically able to work with categorical features, scikit-learnâ€™s current implementation **does not support them natively.**

#### Conclusion & Recommendation

Because ZIP codes are high-cardinality categorical variables, **Râ€™s** native support for categorical splitting gives it a strong advantage for interpretable trees based on ZIP.

In **Python**, you can model ZIP, but you must choose encoding carefully (one-hot, target encoding, etc.) knowing that it will influence how splits are made and how feature importance is calculated.

:::


3. **Are There Any Suggestions for Implementing Decision Trees in Python With Proper Categorical Handling?** Please poke around the Internet (AI is not as helpful with new libraries) for suggestions on how to implement decision trees in Python with better (i.e. not one-hot encoding) categorical handling.  Please provide a link to the source and a quote from the source.  There is not right answer here, but please provide a thoughtful answer, I am curious to see what you find.

::: {.callout-note appearance="minimal"}

If you're working in a Python environment but need models that capture ZIP-codeâ€“level effects, consider using advanced encoding techniques (such as target encoding or ordinal encoding with level grouping) or choose algorithms that natively support categorical featuresâ€”like CatBoost, XGBoost, LightGBM, or H2O.

Some of the leading Python libraries with strong support for categorical variables include:

* ##### CatBoost (Yandex)

**CatBoost** is a gradient boosting library from Yandex designed to handle **categorical variables natively**, without requiring one-hot encoding or manual preprocessing. It uses advanced techniques like **ordered boosting** and **target-based statistics** to convert categorical features into powerful numeric representations during training. This allows CatBoost to capture category interactions, avoid target leakage, and provide strong performance even with high-cardinality variables such as ZIP codes. CatBoost works efficiently out of the box, often requiring minimal tuning, and typically outperforms traditional boosting methods when datasets contain many categorical features.

* ##### LightGBM (Microsoft)

**Category support:** Yes (via native categorical handling)
LightGBM is a gradient boosting framework that supports categorical features natively by using **optimal split techniques** (based on category frequency and gradient statistics). You simply specify which columns are categorical, and the model handles them efficiently without one-hot encoding. It is extremely fast and works well with large datasets.

* ##### XGBoost

**Category support:** Yes (newer versions; previously no native categorical support)
XGBoost added **experimental categorical handling** through **one-hot encoding internally** or via **partition-based splits**. Earlier versions required manual encoding, but newer releases allow passing categorical columns directly, though performance is still not as strong as CatBoost or LightGBM for high-cardinality features.

* ##### H2O.ai Gradient Boosting / AutoML

**Category support:** Yes (very strong categorical support)
H2Oâ€™s machine learning platformâ€”including Gradient Boosting, Random Forest, and AutoMLâ€”supports categorical variables directly using **distributed one-hot encoding, impact encoding, and categorical grouping.** It is scalable, memory-efficient, and well-suited for large datasets.

* ##### PySpark MLlib (Spark)

**Category support:** Yes (through built-in transformers)
Spark MLlib handles categorical data using tools like **StringIndexer, OneHotEncoder,** and **VectorAssembler**. While not as automated as CatBoost, it is designed for distributed computing and is commonly used for very large datasets.

* ##### RAPIDS cuML (NVIDIA)

**Category support:** Yes (GPU-accelerated categorical encoders)
cuML provides GPU-accelerated tree-based models (similar to XGBoost/LightGBM) and includes fast categorical encoders such as **TargetEncoding, OneHotEncoding, and OrdinalEncoding,** making it suitable for large datasets and real-time processing on GPUs.

References..

[https://proceedings.neurips.cc/paper_files/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf)

:::

